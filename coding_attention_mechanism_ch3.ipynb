{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid grammatical structures in the source and target language, Encoder and Decoder are used in deep neural network. The job of Encoder is to first read in and process the entire text, and Decoder then produces the translated text.\n",
    "\n",
    "Before Transformers, RNNs (recurrent neural network) were most popular encoder-decoder architecture for translation. RNN is a neural network in which outputs from previous steps are fed as inputs to the current step, making sequential data like text. \n",
    "\n",
    "Limitation of RNN - We can not access eariler hidden states from the encoder during decoding because it only depend on current state hidden state. This leads to a loss of context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Self-attention` is a mechanisam that allows each position in input sequence to consider the relevancy of all other position in the same sequence when computing repersentation of a sequence. It is key component of LLMs based on transformation architecture. \n",
    "\n",
    "We have covered Input text and Preprocessing part, Now we will go onto Self-attention module.\n",
    "\n",
    "#### Attending to different parts of the input with serlf-attention\n",
    "\n",
    "In self-attention, the “self” refers to the mechanism’s ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
    "\n",
    "\n",
    "#### Self- attention mechanism without trainable weights\n",
    "\n",
    "Notation used - x is input sequence with T elemenets, represented as x^(1) to x^(T). Our goal is to find `Context Vector` z^(i) for each element x^(i).\n",
    "\n",
    "A context vector can be seen as enriched embedding vector. These vector play important role in Self-attention mechanism. They create representation of each element in input sequence by incorporating information from all other elements in the sequence. And later trainable weights are added that help LLM learn to construct these context vector.\n",
    "\n",
    "Let's try to create a simple self-attention mechanism-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take example from book - Your journey starts with one step - our input sequence with 6 elements\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first step is to compute intermediate valeus w called as `attention scores`. Why? Beacuse due to values like 0.87 or 0.85 input tensor truncate them to 0.8. So, from above input embeddings of words \"journey\" and \"starts\" may appear similar.\n",
    "\n",
    "We calculate these scores by dot product. If query token is 2nd token x^(2), all w will be calulated by dot prodcut with this embedded query token. (w21, w22, w23....w2T)\n",
    "\n",
    "let's see this concept-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token as query, see inputs above\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now next step is `Normlaization`, here goal is to obtain attention weigths that sum upto 1. It is useful for interpretation and maintaining training stability in LLM.\n",
    "\n",
    "We will get alpha values for each attention score, these are nothing but `Attention weights`. So, we normalize w to get alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_scores_2_tmp)\n",
    "print(\"Sum: \", attn_scores_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, `Softmax` function is used. AS it manages extreme values and offers more favorable gradient properties during training. \n",
    "\n",
    "Softmax function also ensures that the attention weights are always positive. This makes output interpretable as probabilities where higher weights indicate greater importance. \n",
    "\n",
    "Let's see this-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid numerically instability that causes overflow and underflow duwe to large or small values, we use PyTorch . Here, our case is small, so it will give same output as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated normalized attention weights, we will go for final step, calculating the context vector z^(2) by multiplying the embedded input tokens x^(i) with corresponding attention weights and summing them to get resulting vectors.\n",
    "\n",
    "So we can say, context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can see a example that explain it step by step:\n",
    "\n",
    "inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "attn_weights_2 = torch.tensor([0.1, 0.2, 0.7])\n",
    "\n",
    "Here's what happens step-by-step:\n",
    "\n",
    "query = inputs[1]: query is assigned the value [3.0, 4.0].\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape): context_vec_2 is initialized as a tensor [0.0, 0.0].\n",
    "\n",
    "The for loop iterates over each input vector:\n",
    "\n",
    "* For i = 0, x_i = [1.0, 2.0], the weighted vector is 0.1 * [1.0, 2.0] = [0.1, 0.2]. So, context_vec_2 becomes [0.1, 0.2].\n",
    "* For i = 1, x_i = [3.0, 4.0], the weighted vector is 0.2 * [3.0, 4.0] = [0.6, 0.8]. So, context_vec_2 becomes [0.1 + 0.6, 0. 2 + 0.8] = [0.7, 1.0].\n",
    "* For i = 2, x_i = [5.0, 6.0], the weighted vector is 0.7 * [5.0, 6.0] = [3.5, 4.2]. So, context_vec_2 becomes [0.7 + 3.5, 1.0 + 4.2] = [4.2, 5.2].\n",
    "* print(context_vec_2) prints the tensor [4.2, 5.2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generalize above process- \n",
    "\n",
    "##### Computing attention weights for al input tokens\n",
    "\n",
    "From our inputs, we have 6X6 tensor and we saw all the steps above - so make some modification to code to get all the context vector instead of z^(2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
